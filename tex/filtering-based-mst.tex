\section{Filtering-based MST Algorithm}\label{sec:kmachine-filtering-mst}
In this subsection, we analyze an algorithm to find a minimum spanning tree (MST) using a filtering technique to eliminate edges   (which was also used  in~\cite{LMSV11}). 

We use three procedures in the course of this algorithm. The first procedure is the well known Kruskal's algorithm  (refer to~\cite{CLRS09}), which when run on a graph with $m$ edges outputs a minimum spanning tree in $\bigO{m\log{m}}$ rounds. 

The second procedure is used to perform matching and is run locally on a machine. It takes a clique of $n$ nodes with unique IDs and outputs a maximal matching in $\bigO{n\log{n}}$ local computation time. First, sort IDs in ascending order using the commonly used MergeSort~\cite{CLRS09}. Then for $i=1$ to $\floor{\sfrac{n}{2}}$, match the $(2i -1)^{th}$ and $(2i)^{th}$ nodes in the sorted list.

The third is the following distributed routing on a clique with $n$ nodes. If each node $u$ wants to send $f$ messages to some other node $\pi(u)$ such that for all $u \neq v, \pi(u) \neq \pi(v)$, then all messages from node $u$ can be sent to $\pi(u)$ in $\bigO{f/(n-1)}$ rounds. The procedure takes two phases. In the first phase, each node $u$ divides its messages into groups of $f/(n-1)$ messages (and appends the destination ID of $\pi(u)$ to each message) and sends these groups of messages across each of its edges, one group per edge, in $\bigO{f/(n-1)}$ rounds. In the second phase, each node forwards whatever messages it received in phase one to the target node in another $\bigO{f/(n-1)}$ rounds.\footnote{The reason we are able to avoid more complicated routing procedures is because (i) each node $u$ sends all its messages to exactly one $\pi(u)$ and (ii) for all $u \neq v, \pi(u) \neq \pi(v)$. As a result, for each edge in the graph, the messages in phase one can be attributed to initial sources and the messages in phase two can be attributed to final destinations. Since each node of an edge can be a source or a destination for exactly one set of $\bigO{f/(n-1)}$ messages, the total number of messages flowing through that edge in either phase is $\bigO{f/(n-1)}$.}

Each machine $M$ maintains graph $G_M$ initially comprising the subgraph induced by edges in $M$, as well as an ordered list $H_M$ initially comprising the IDs of all machines. Each machine $M$ executes at most $2 \ceil{\log k}$ phases of the following steps until termination:
\begin{enumerate}
    \item If $M$ received information about new edges in the previous phase, it updates $G_M$ accordingly. Run Kruskal's algorithm on $G_M$ to remove cycles, if any.
	\item If $|H_M| = 1$, terminate. Use the matching procedure on $H_M$ to match with another machine $M'$. If $M$ matches with $M'$ and the ID of $M$ is greater than the ID of $M'$, use the routing procedure to send $G_M$ to $M'$.
    \item For all machines $M'' \in H_M$ where $M''$ matched with another machine having a lower ID, remove $M''$ from $H_M$. If $M$ was one such $M''$, then terminate.
\end{enumerate}

At the end of algorithm, the machine $M_{min}$ with the lowest ID will have the full MST stored in its $G_{M_{min}}$. The following theorem captures the properties of this algorithm.

\begin{theorem}
    \label{thm:filtering-thm}
	The above algorithm results in the machine with the lowest ID $M_{\min}$ having the full MST stored in its graph $G_{M_{\min}}$ with $T_{\ell} = \bigO{\rho\log{\rho} + n\log{n}\log{k}}$ with high probability, where $\rho = m/k + \Delta \log n$, and $T_c = O(n\log k/k)$.
\end{theorem}

\begin{proof}
We first argue the correctness of the algorithm. In particular, we show that, eventually, information about every edge in the MST of the input graph will be sent to machine $M_{\min}$ before all phases end. Once $M_{\min}$ knows of all these edges, Kruskal's algorithm guarantees that the correct MST will be computed. 

Consider some edge, $e$, of the graph that belongs to the MST. Let $e$ reside on some machine $M$. Because $e$ is an edge of the MST of $G$, $e$ will also be an edge of the MST of any subgraph of $G$ that contains $e$. As a result, $e$ will never be removed by $M$ or any subsequent machine it is located on when Kruskal's algorithm is run. Furthermore, by the matching algorithm, we see that regardless of the initial machine $e$ is located on, after $2 \ceil{\log k}$ phases, it will eventually make its way to machine $M_{\min}$. Thus the algorithm works as intended.

To bound the local computation complexity, observe that any machine $M$ runs for at most $\bigO{\log k}$ phases. 
In any phase, $M$ performs Kruskal's algorithm on graph $G_M$. In the $k$-machine model, by
\cref{lem:more-acc-mapping-lemma}, each machine initially receives $\bigO{m/k + \Delta \log n} = \bigO{\rho}$ edges
with high probability
 resulting in a run time of $\bigO{\rho \log \rho}$ in the first phase. Subsequently, since there are $n$ nodes in the input graph, there can be at most $n-1$ edges chosen by $M$ (or any other machine) to be sent over in that phase. Thus, $M$ processes at most $2n-2$ edges spanning at most $n$ nodes in each subsequent phase resulting in an additional run time of $\bigO{n \log n}$ per phase. Also in each phase, $M$ runs the matching procedure on $H_M$ resulting in at most $\bigO{k \log k}$ steps of local computation. Totally, each machine takes $\bigO{\rho \log \rho + (n \log n  + k \log k)\log k} = \bigO{\rho \log \rho + n \log n \log k}$ steps of local computation with high probability. Note that there is an additive $k$ that comes due to a machine possibly having to send/read inputs from all other machines, but this is subsumed by the larger terms in the complexity.

For the communication complexity, notice as described in the previous paragraph that each machine only needs to send at most $n-1$ edges to another machine it matches with in any give phase. From the matching procedure, this takes $\bigO{n/k}$ rounds per phase. Furthermore, there are at most $\bigO{\log k}$ phases, resulting in a communication complexity of $\bigO{n\log k/k}$.
\end{proof}
