\section{Preliminaries}

\subsection{Node Distribution Lemma}\label{sec:conv-the}

The Node Distribution Lemma gives a way to bound the parameters associated with
the vertices of an input graph $G$ when it is mapped to the $k$-machine model via
the random vertex partition (RVP) model, where each vertex of the input graph $G$
is assigned independently and uniformly at random among the $k$ machines.
 The parameter of interest can be the degree associated with a vertex or the local computation
cost of a vertex (in the standard \textsc{Congest} model).

\begin{lemma}[Node Distribution Lemma]
    \label{lem:node-distribution-lemma}
    Consider a graph \(G\) of nodes \(v_1, v_2, \hdots, v_n\) with associated non-negative real-valued ``weights'' \(w(v)\) for each node \(v\). Given a uniform, random distribution of the \(n\) nodes to \(k\) machines, as in the \(k\)-machine model, then, with probability at least $1 - \sfrac{1}{n^a}$ for any $a > 0$, the total weight of nodes at every machine is bounded above by $\bigO{T_{\text{avg}}+\log{n}\cdot w_{\max}}$, where \(T_{\text{avg}}=\frac{1}{k}\sum_{i=1}^n w(v_i)\) and \(w_{\max}=\max\set{w(v_i)}\).
\end{lemma}

\begin{proof}
    Partition the nodes into buckets \(\beta_i, 1 \leq i \leq B=\ceil{\log{w_{\max}}} + 1\) based on weight as follows:
	\[\beta_i = \set{v \mid 2^{i-1} \leq w(v) < 2^i}\]
	and set \(\beta_0=\set{v \mid 0 \leq w(v) < 1}\). 
	Furthermore, define \(n_i=\card{\beta_i}\) and let \(B=\ceil{\log{w_{\max}}} + 1\) denote the number of buckets.

	Fix a machine and let \(\mathcal{M}\) denote the set of nodes mapped to it. Furthermore, fix a bucket \(\beta_i=\set{u_1,u_2,\hdots,u_{n_i}}\) and define the random variable \(X_{i,j}\) as
	\[X_{i,j}=\begin{cases}
		1 & \text{ if } u_j \in \mathcal{M}\\
		0 & \text{ otherwise}
	\end{cases}\] Set \(X_i=\sum_{j=1}^{n_i} X_{i,j}\) and note that \(\expectation{X_{i,j}}=\frac{1}{k}\), hence \(\expectation{X_i}=\frac{n_i}{k}\).
	Now, define \[W_i=\sum_{j=1}^{n_i}X_{i,j}w(u_j)\] i.e., \(W_i\) denotes the total weight of nodes in bucket \(\beta_i\) from machine \(M\), and note that \[\prob{W_i > t2^i} \leq \prob{X_i >t}\] Thus, taking \(t = 6\expectation{X_i} + c\log{n}\), we have
	\begin{align*}\prob{W_i > \left(6\expectation{X_i} + c\log{n}\right)2^i}
		&\leq \prob{X_i > 6\expectation{X_i} + c\log{n}}\\
		&\leq 2^{-\left(6\expectation{X_i} + c\log{n}\right)}~\text{\cite{MU17}}\\
		&\leq 2^{-6\expectation{X_i}}\frac{1}{n^c}
	\end{align*}
	Now, notice that
	\begin{align*}\sum_{i=0}^B \left(6\expectation{X_i} + c\log{n}\right)2^i
		&=    \sum_{i=0}^B 6\cdot2^i\frac{n_i}{k} + c\log{n}\sum_{i=0}^B 2^i\\
		&\leq 12\sum_{i=1}^n\frac{w_i}{k} + 4c\log{n}\cdot w_{\max}\\ 
		&=   12T_{\avg} + 4c\log{n}\cdot w_{\max}\\
	\end{align*}
	Taking a union bound across all buckets, we have
	\begin{align*}&\prob{\sum_{i=0}^B W_i > 12T_{\avg} + 4c\log{n}\cdot\ell_{\max}}\\
		&\leq \prob{\sum_{i=0}^B W_i > \sum_{i=0}^B\left(6\expectation{X_i} + c\log{n}\right)2^i}\\
		&\leq \prob{\sum_{i=0}^B X_i > \sum_{i=0}^B 6\expectation{X_i} + c\log{n}}\\ 
		&\leq \sum_{i=0}^B 2^{-6\expectation{X_i}}\frac{1}{n^c}\\
		&\leq \frac{1}{kn^a}\text{ for sufficiently large }c
	\end{align*}
	Taking union bound again across all machines, the probability that any machine has total weight greater than \(12T_{\avg} + 4c\log{n}\cdot w_{\max}\) is bounded above by \(\sum_{i=1}^k\frac{1}{kn^a}=\sfrac{1}{n^a}\). Thus, with probability at least \(1 - \sfrac{1}{n^a}\), every machine has a total weight less than \(12T_{\avg} + 4c\log{n}\cdot w_{\max}\), i.e., all machines have total weight \(\bigO{T_{\avg} + \log{n}\cdot w_{\max}}\) with high probability. 
\end{proof}

\subsection{A More Accurate Mapping Lemma \& Conversion Theorem}\label{sec:more-acc-mapping-lemma}
In this section, we reanalyze the Mapping Lemma from~\cite{KlauckNPR15} (Lemma~4.1 in the reference) in order to develop more exact bounds. The Mapping Lemma gives a bound on the  number 
of vertices and  edges of the input graph $G$ that are mapped to the $k$ machines, assuming
the RVP model. It also gives a bound on the number of edges of $G$ assigned to a link in the $k$-clique. While the first two bounds (the number of vertices and number of edges assigned)
is the same as in \cite{KlauckNPR15}, the bound on the number of edges per link
as stated  and analyzed in \cite{KlauckNPR15} is not fully correct (there the bound was
\(\bigO{m/k^2 + \Delta/k}\), whereas here we show \(\bigO{m/k^2 + n/k}\)).  The analysis in~\cite{KlauckNPR15} also yielded values with hidden polylog terms. We show that a more careful analysis results in no polylog terms. We present the lemma below. 
 The proof uses a powerful (and not well-known) concentration inequality due
to Rodl and Rucinski \cite{RR94} (as used in \cite{PRS21}) which can be of independent interest.

\begin{lemma}[Mapping Lemma]
    \label{lem:more-acc-mapping-lemma}
Let an \(n\)-node, \(m\)-edge graph \(G\) be partitioned among the \(k\) machines as \(N = \set{p_1, \ldots, p_k}\), according to the random vertex partition model (assume \(k = o\left(n\right)\)). Then with probability at least \(1 - 1/n^\alpha\), where \(\alpha > 1\) is an arbitrary fixed constant, the following bounds hold:
\begin{enumerate}
    \item The number of vertices mapped to any machine is \(\bigO{n/k}\).
    \item The number of edges mapped to any machine is \(\mathcal{O}(m/k + \Delta \log n)\).
    \item The number of edges mapped to any link of the network is \(\bigO{m/k^2 + n/k}\).
\end{enumerate}
\end{lemma}

\begin{proof}
    To prove the first bound, recall that nodes are distributed uniformly at random over the machines. Therefore, on expectation, each machine has \(n/k\) nodes. Since $k = \bigO{n^{\epsilon}}$, where $0 < \epsilon < 1$ is a constant, \(n/k = \mathcal{O}(n^{\Omega(1)})\). Thus, we can directly apply a Chernoff bound (the third inequality of Theorem 4.4. from~\cite{MU17}) to see that each machine gets no more than \(6n/k =  \bigO{n/k}\) vertices with the desired probability.

    To prove the second bound, we look back at Lemma~\ref{lem:node-distribution-lemma} and its proof in this paper. If we consider the weights associated with each node to be the local degree of each node, the proof follows directly and we get the desired bound.
    
    To prove the third bound, we utilize the following proposition from~\cite{PRS21}, itself a more accurate version of a proposition from~\cite{RR94}.
    
    \begin{proposition}[Proposition~2 in~\cite{PRS21}]
        \label{prop:edge-bound}
    Let \(G\) be a graph with \(m < \eta n^2\) and let \(R\) be a random subset of \(V\) of size \(\card{R} = t\) such that \(t \geq 1/3\eta\). Let \(e(G[R])\) denote the number of edges in the subgraph induced by \(R\). Then,
        $\prob{e(G[R]) > 3 \eta t^2} < t \cdot e^{-ct}$
    for some \(c > 0\).
    \end{proposition}
    
    Setting \(\alpha = 2\) in our first bound, we see that with probability \(1 - 1/n^2\), each machine gets a random subset of no more than \(6n/k\) nodes. Now, it is easy to see that an upper bound on the number of edges formed by a graph of \(12n / k\) nodes acts as an upper bound on the number of edges mapped to a link between any two machines. We first calculate this upper bound on one link and subsequently extend that bound to all links. We can derive this upper bound by using
    Proposition~\ref{prop:edge-bound} with values \(t =  12n/k\) and \(\eta = m/n^2 + k/n\). Using the proposition, we see that \(\prob{e(G[R]) > O(m/k^2 + n/k)} < 1/n^{\alpha+1}\) for a sufficiently large \(n\).
    
    Now that we have the bound on the number of edges mapped to one link with high probability, we use a simple union bound to see that the number of edges mapped to any link is no more than \(\bigO{m/k^2 + n/k}\) with probability \(1-1/n^\alpha\).
\end{proof}


We are also able to get a more accurate version of the Conversion Theorem from~\cite{KlauckNPR15} (Theorem~4.1 in the reference). As the only changes in the proof are the addition of a \(\log{n}\) factor to account for when \(W=1\) and the direct utilization of the updated bounds from Lemma~\ref{lem:more-acc-mapping-lemma},
 we omit the proof here.

\begin{theorem}[Conversion Theorem]
    \label{thm:more-acc-conversion-theorem}
Suppose that there is an \(\epsilon\)-error algorithm \(A_C\) that solves problem \(P\) in time \(T_C(n) \in \softO{n}\) in the \textsc{Congested Clique} model, for any \(n\)-node input graph. Then there exists an \(\epsilon\)-error algorithm \(A\) that solves \(P\) in the \(k\)-machine model with bandwidth \(W\) satisfying the following time complexity bounds with high probability:
\begin{enumerate}
    \item If \(A_C\) uses point-to-point communication with message complexity \(M\), then \(A\) runs in \(\bigO{(\frac{M}{k^2 W} + T_C(n)\ceil{\frac{n}{kW}}) \log n}\) time.
    \item If \(A_C\) is a broadcast algorithm with broadcast complexity \(B\), then \(A\) takes \(\bigO{\frac{B}{kW} \log n + T_C(n)}\) time.
\end{enumerate}
\end{theorem}
