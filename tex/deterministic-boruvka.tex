\section{Deterministic Bor\r{u}vka-style Algorithms for MST}\label{sec:boruvka}
In this section, we present two \emph{deterministic} distributed algorithms for MST that both have (communication) round complexity of \(\softO{\sfrac{n}{k}}\) and have increasingly better local computation complexity.\footnote{If the graph is connected, the algorithms in this section find an MST. If the graph is disconnected, the algorithms produce a minimum spanning forest (MSF). Note that in an MSF, the number of edges $m$ may be less than the number of nodes $n$.} The  first algorithm (Simple Local Bor\r{u}vka) takes \(\softO{\sfrac{m}{k} +  n}\) local computation time while the second algorithm (Improved Local Bor\r{u}vka) takes \(\softO{\sfrac{m}{k} + \Delta + k}\). 
Note that $\softO{\sfrac{n}{k}}$
is the best possible round complexity if each machine is required to know the MST edges incident on
the vertices assigned to it~\cite{KlauckNPR15}.\footnote{However, if any machine can output any of the MST edges, then the lower bound is $\softOm{n/k^2}$, also shown in \cite{KlauckNPR15}.}
The algorithms are based on a distributed implementation of Bor\r{u}vka's algorithm \cite{Boruvka26,dnabook},  but implemented specifically for the \(k\)-machine model. (Note
that the classical GHS algorithm \cite{GallagerHS83} is a distributed implementation of Bor\r{u}vka in the standard \textsc{Congest} model.) Both algorithms follow a Bor\r{u}vka-style strategy~\cite{Boruvka26}, i.e., repeatedly merge adjacent \emph{components} of the input graph \(G\), which are connected subgraphs of \(G\), to form larger (connected) components (e.g., \cite{dnabook}).

A Bor\r{u}vka-style algorithm proceeds in \emph{phases}. In the first phase, the  algorithm starts with each individual node as a fragment by itself. In subsequent phases, fragments are merged until there is only one remaining --- the MST. That is, at the beginning, there are \(n=\card{V}\) fragments, and at the end of the final phase, a single fragment which is the MST. Throughout, we assume without loss of generality that all edge weights are distinct (if there are duplicate weights, ties can be broken by using the IDs of the endpoints).
At a high level, both algorithms do the following  in each phase:
\begin{enumerate}
    \item For every fragment, its minimum outgoing edge (MOE) is found. An outgoing edge of a fragment is an edge that connects a node of the fragment to a node of a \emph{different} fragment
          and an MOE is an outgoing edge of the least weight. By the cut property, all MOE edges are in the MST. % \cite{}.
    \item Fragments are merged along their MOE edges.
\end{enumerate}
The phases are repeated until all nodes belong to the same fragment.

From the \nameref{lem:more-acc-mapping-lemma}, we note that each machine initially has \(n/k\) nodes distributed randomly (the RVP model), as well as their adjacency lists, and that
the number of edges assigned to each machine is $\bigO{m/k + \Delta \log n}$.  \\

\noindent \textbf{Simple Local Bor\r{u}vka Algorithm.}
Initially, each machine sorts the adjacency list of each node by edge weight, then creates a disjoint-set (or union-find) data structure \cite{CLRS09}, \(\mathcal{M}\), of \emph{fragments}, with \emph{each of the \(n\)} nodes initially belonging to its own fragment. The disjoint-set admits the following operations: (1) \(\operatorname{MakeSet}\) - adds an element to the set in \(\bigO{1}\) time. (2) \(\operatorname{Find}\) - determines which disjoint set an element belongs to in \(\bigO{\invack{n}}\) amortized time (where $\invack{n}$ is the inverse Ackermann function). (3) \(\operatorname{Union}\) - combines disjoint sets in \(\bigO{\invack{n}}\) amortized time.



We note that each machine can easily determine the identities of all \(n\) nodes in \(\bigO{n/k}\) rounds (by executing \(n/k\) broadcasts).

Each phase consists of the following two steps:\\
\textbf{Step 1:} For each vertex \(v\) in machine \(M\), \(M\) iterates through the adjacency list of \(v\) and performs \(\operatorname{Find}\) on each of its neighbors, in increasing order of edge-weight, to determine if the incident edge belongs to a different fragment. Once such an edge is found, \(M\) stops searching and broadcasts this edge to all machines. Note that this search is amortized \(\bigO{\deg{v}}\) local time, as an edge never needs to be checked twice.

Machine $M$ now has a list of candidates for MOE for each fragment. Next, \(M\) notes the minimum candidate for each fragment by iterating through all candidates for each fragment, which it has received from the broadcast mentioned above.\\
\textbf{Step 2:} \(M\) merges fragments: if \((u, v)\) is some minimum outgoing edge between two fragments, \(M\) updates \(\mathcal{M}\) with \(\operatorname{Union}(u, v)\).

At the end of this algorithm, each machine has a local copy of the complete MST.
\begin{theorem}
    \label{thm:gh1}
    The above algorithm ends with \(\mathcal{M}\) as the MST and, with high probability (with respect to the RVP model), we have $T_{\ell} = \softO{\sfrac{m}{k} + n}$ and $T_c = \softO{\sfrac{n}{k}}$.
\end{theorem}


\begin{proof}
    First, note that correctness of this algorithm follows directly from the correctness of Bor\r{u}vka's algorithm.


    Next, note that at the end of each phase, every fragment will merge with at least one other fragment. In the worst case, the number of fragments is halved, hence there are \(\bigO{\log{n}}\) phases, and each phase requires at most \(\bigO{n/k}\) rounds, for a total of $T_c=\bigO{\sfrac{n}{k}\log{n}}=\softO{\sfrac{n}{k}}$.

    Finally, we show the local computation bound. Fix a machine \(M\) and note that pre-processing requires $\bigO{\deg{v}\log{\deg{v}}} = \bigO{\deg{v}\log{\Delta}}$ per node $v$ in \(M\) to sort the adjacency lists. Moreover, to create \(\mathcal{M}\) requires \(\bigO{n}\). In total, this is \(\bigO{\left(\sfrac{m}{k}+\Delta\log{n}\right)\log{\Delta} + n}\) with high probability. The local computation involved in finding the local MOE from Step 1 is amortized \(\bigO{\invack{n}\deg{v}}\) 
    per node, which (by the Mapping Lemma) is \(\bigO{(\sfrac{m}{k} + \Delta \log n) \alpha(n)}\) in total. Finding the MOE per fragment is an additional \(\bigO{n}\) operations. A single instance of Step 2 in a single phase requires \(\bigO{\invack{n}}\) and there can be at most $n-1$ merge operations. Finally, we may have an additive $k$ from a machine possibly having to send/read inputs from all other machines. Thus, we have
	$T_{\ell}=\mathcal{O}(\sfrac{m}{k}\log{\Delta} + \Delta\log{\Delta}\log{n} + n + (m \alpha(n)/k)\linebreak + \Delta \alpha(n) \log n  + n + n \alpha(n)  + k) = \softO{\sfrac{m}{k} + n}$.\qedhere
\end{proof}


\noindent \textbf{Improved Local Bor\r{u}vka Algorithm.}\label{subsec:improved-boruvka}
We can improve the local computation complexity of the Simple Local Bor\r{u}vka algorithm
(which is linear in $n$)  by using a different implementation of Bor\r{u}vka where instead
of broadcast, we simulate the ``unicast" version of Bor\r{u}vka --- as in the GHS algorithm
in the standard \textsc{Congest} model, where  messages are sent through the edges of the graph.
We then use the \nameref{thm:more-acc-conversion-theorem} to compute
the round complexity. However, to get a $\softO{\frac{n}{k}}$ bound (independent of $m$), we first apply filtering to reduce
the number of edges in each machine to $\bigO{n}$ (i.e., $\bigO{nk}$ edges overall) which can be done as follows.

The machines begin by creating a minimum spanning forest (MSF) of their subgraphs using, for example, Kruskal's Algorithm, and essentially discarding edges that are not part of their local MSFs. The edges remaining (of which there are at most \(k\left(n - 1\right)=\bigO{nk}\) with high probability by the Mapping Lemma) are then the only candidates for the MST of the original graph. Moreover, each link between machines corresponds to at most \(\bigO{\sfrac{n}{k}}\) graph edges. The algorithm then continues on these remaining subgraphs.

Like Simple Local Bor\r{u}vka, we proceed in phases. Fix a machine \(M\). To determine the MOE for a fragment \(f\), we first determine the local minimum outgoing edge (LOE) using the following steps. Start with \(\mathrm{LOE}=\infty\), then:\\
\textbf{Step 1:} Iterate through the edges in \(f\). For an edge \((u, v)\) with \(u\) in \(M\), send a message to the machine containing \(v\) with the IDs of \(u\) and \(v\) and the fragment ID of \(f\).\\
\textbf{Step 2:} For each received message \((u, v, f)\), if the fragment ID of \(v\) is different than the received fragment ID, respond with the original fragment ID and the fragment ID of \(v\).\\
\textbf{Step 3:} Update \(\mathrm{LOE} = \min\left(\mathrm{LOE}, w\right)\) for each message received corresponding to fragment \(f\), where \(w\) is the weight of the outgoing edge.\\
\textbf{Step 4:} Broadcast the LOE and fragment ID.

From the broadcast of the LOEs,  each machine locally determines the global MOE for any fragments it contains.

The machine then merges fragments as follows. Note that one cannot merge all fragments,
since that takes time proportional to the length of the fragment chain. To address
this, we use a technique that is similar to (but simpler than) the \emph{controlled GHS} algorithm~\cite{dnabook}.
Create a rooted tree \(F\) where each node is a fragment and there exists an edge between two fragments if there is an MOE between them. Construct a maximal matching (using e.g., Cole-Vishkin~\cite{CV86}) and merge all matched edges \emph{and} any edge where \emph{exactly one} endpoint is matched.

\begin{theorem}
    \label{thm:ghs2}
    With high probability, the above algorithm satisfies $T_{\ell} = \softO{\sfrac{(m + n)}{k} + \Delta + k}$ and $T_c = \softO{\sfrac{n}{k}}$.
\end{theorem}

\begin{proof}
    For local computation, the filtering will require $\mathcal{O}((\sfrac{m}{k}+\Delta)\log{\sfrac{n}{k}})$.
We note that the Cole-Vishkin algorithm (per phase) takes $\bigO{\log^*n}$ rounds
    in the standard \textsc{Congest} model which can be simulated in the $k$-machine model in $\bigO{(n/k)\log^*n}$ rounds since each machine simulates the algorithm for each of
    the $\bigO{n/k}$ vertices assigned to it (apply the broadcast version of
    the \nameref{thm:more-acc-conversion-theorem}). Its local computation cost
    is $\bigO{(n/k)\log^*n}$ since the simulation is done on the MOE edges only, which is $\bigO{n/k}$
    per machine. Additionally, we have an additive $k$ that comes due to a machine possibly having to send/read inputs from all other machines.

    In order to bound the number of total phases, we prove the following lemma.
    \begin{lemma}
        At any step in the given algorithm, the number of sequential merges is at most 3, and the number of total fragments is reduced by at least half after each merge.
    \end{lemma}

    \begin{proof}

        To see that the number of sequential merges is at most 3, simply enumerate all possible labelings of edges (matched or unmatched) on four fragments \(f_1\), \(f_2\), \(f_3\), \(f_4\), with edges from \(f_1\) to \(f_2\), \(f_2\) to \(f_3\), and \(f_3\) to \(f_4\), as well as with one ``outgoing'' edge from each.

        To see that the total number of fragments is reduced by at least half, notice that \emph{every} fragment is part of a merge, for if some fragment \(f\) is not part of a merge, then none of its incident edges are part of the matching \emph{and} none of the edges incident to its neighbors are part of a merge. In that case, we can match an additional edge between the fragment and one of its neighbors, contradicting the maximality of our matching.
    \end{proof}

    By the previous lemma, there are \(\bigO{\log{n}}\) phases in total. Per phase, the overall local computation is clearly $\softO{\sfrac{(m + n)}{k}+\Delta + k}$. In total, this is $\softO{\sfrac{(m+n)}{k}+\Delta + k}$.

    For round complexity, we note that all communication is either via the graph edges
    (as simulated in the $k$-machine model) or by doing $O(n/k)$ broadcasts per phase.
    Round complexity is now obtained by using the Mapping Lemma (where $m = O(nk)$ by filtering) and the Conversion Theorem.
\end{proof}